{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mzJ8F_21RM12"
      },
      "source": [
        "**Part-2 (Multiclass Classification):**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mIB-Eky6RMd-",
        "outputId": "525dbad8-6b85-4a00-d721-4c85dc0da811",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "#2.1.a) Use the data from the file reduced_mnist.csv in the data directory. Begin by reading the data.\n",
        "#Print the following information:\n",
        "#Number of data points\n",
        "#Total number of features\n",
        "#Unique labels in the data\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "# import warnings filter\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "\n",
        "df = pd.read_csv('reduced_mnist.csv')\n",
        "m,n=df.shape\n",
        "#print(\"Data shape: {}\".format(df.shape))\n",
        "print('The number of records are :',m)\n",
        "print('The number of columns are :',n)\n",
        "print('The unique labels in the dataset are ',sorted(df.label.unique()))\n",
        "\n",
        "print('')\n",
        "print(\"\"\"The MNIST dataset contains 2520 records and 785 columns\n",
        " * 784 feature columns pixel0 to pixel783\n",
        " * 1 label column\"\"\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of records are : 2520\n",
            "The number of columns are : 785\n",
            "The unique labels in the dataset are  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "\n",
            "The MNIST dataset contains 2520 records and 785 columns\n",
            " * 784 feature columns pixel0 to pixel783\n",
            " * 1 label column\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NIqT5IAjRKUw",
        "outputId": "689421b1-0bba-4e9c-fe37-e335d5f8a971",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "#2.1 b) Split the data into 70% training data and 30% test data. Fit a One-vs-Rest Classifier (which uses Logistic regression \n",
        "#classifier with alpha=1) on training data, and report accuracy, precision, recall on testing data.Â \n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "# import warnings filter\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "dftrain, dftest = train_test_split(df, test_size=0.3)\n",
        "features_train=dftrain.iloc[:,1:785]\n",
        "labels_train=dftrain.iloc[:,0]\n",
        "features_test=dftest.iloc[:,1:785]\n",
        "labels_test=dftest.iloc[:,0]\n",
        "alpha=1\n",
        "\n",
        "clf = LogisticRegression(random_state=0, multi_class='ovr', C=1/alpha)\n",
        "ovr_model = clf.fit(features_train, labels_train)\n",
        "\n",
        "labels_predicted=ovr_model.predict(features_test)\n",
        "\n",
        "print('Accuracy of One Vs Rest is : '+ str(accuracy_score(labels_test,labels_predicted)))\n",
        "print('')\n",
        "print(classification_report(labels_test,labels_predicted,digits=3))\n",
        "\n",
        "confusion_matrix_ovr = confusion_matrix(labels_test,labels_predicted)\n",
        "print('Here is the confusion matrix')\n",
        "print('----------------------------')\n",
        "print(confusion_matrix_ovr)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of One Vs Rest is : 0.83994708994709\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.862     0.938     0.898        80\n",
            "           1      0.950     0.970     0.960        99\n",
            "           2      0.833     0.783     0.807        83\n",
            "           3      0.812     0.757     0.783        74\n",
            "           4      0.867     0.844     0.855        77\n",
            "           5      0.746     0.794     0.769        63\n",
            "           6      0.840     0.913     0.875        69\n",
            "           7      0.863     0.863     0.863        73\n",
            "           8      0.726     0.692     0.709        65\n",
            "           9      0.826     0.781     0.803        73\n",
            "\n",
            "   micro avg      0.840     0.840     0.840       756\n",
            "   macro avg      0.833     0.833     0.832       756\n",
            "weighted avg      0.839     0.840     0.839       756\n",
            "\n",
            "Here is the confusion matrix\n",
            "----------------------------\n",
            "[[75  1  1  1  0  0  1  0  1  0]\n",
            " [ 0 96  0  0  0  1  0  0  2  0]\n",
            " [ 3  0 65  2  0  0  8  1  3  1]\n",
            " [ 1  0  4 56  1  8  1  0  3  0]\n",
            " [ 0  0  0  2 65  1  1  0  2  6]\n",
            " [ 3  0  1  2  0 50  0  0  4  3]\n",
            " [ 3  0  1  0  1  0 63  0  1  0]\n",
            " [ 0  2  2  4  0  0  0 63  0  2]\n",
            " [ 2  2  3  2  3  7  1  0 45  0]\n",
            " [ 0  0  1  0  5  0  0  9  1 57]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvRrpMT5g27K",
        "colab_type": "code",
        "colab": {},
        "outputId": "7b3a0b37-a7f9-419f-ccb1-620565541ed3"
      },
      "source": [
        "#2.2 Choosing the best hyper-parameter \n",
        "#a) - Choose the best value of alpha from the set a={0.1, 1, 3, 10, 33, 100, 333, 1000, 3333, 10000, 33333} by observing average training and validation performance P.\n",
        "#On a graph, plot both the average training performance (in red) and average validation performacne (in blue) w.r.t. each hyperparameter value.\n",
        "#Comment on this graph by identifying regions of overfitting and underfitting.\n",
        "#Print the best value of alpha hyperparameter\n",
        "\n",
        "# import warnings filter\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def runLROVRmodel(trials, data, penalty_score):\n",
        "\n",
        "   model_acc     = 0\n",
        "   final_f1score_train = np.zeros([1,1])\n",
        "   final_f1score_val = np.zeros([1,1]) # storing model weights\n",
        "    \n",
        "   for i in range(0,trials):\n",
        "       Dtrain, Dtest = train_test_split(data, test_size=0.3)\n",
        "       Xtrain = Dtrain.iloc[:,1:785].copy()\n",
        "       ytrain = Dtrain.iloc[:,0].copy()\n",
        "       Xtest = Dtest.iloc[:,1:785].copy()\n",
        "       ytest = Dtest.iloc[:,0].copy()\n",
        "    \n",
        "       lr = LogisticRegression(C=1/penalty_score, random_state=0, multi_class='ovr')\n",
        "       lr.fit(Xtrain, ytrain)\n",
        "       \n",
        "       y_predict_train = lr.predict(Xtrain)\n",
        "       f1score_train=f1_score(ytrain, y_predict_train, average='weighted')  \n",
        "       final_f1score_train  +=  f1score_train\n",
        "    \n",
        "       y_predict_val = lr.predict(Xtest)\n",
        "       f1score_val=f1_score(ytest, y_predict_val, average='weighted')\n",
        "       final_f1score_val  +=  f1score_val\n",
        "   \n",
        "   final_f1score_train /= trials\n",
        "   final_f1score_val /= trials\n",
        "   return np.round(final_f1score_train, decimals=2),np.round(final_f1score_val, decimals=2)\n",
        "\n",
        "print('We are going to use P as f1-score since fID=',218545396%3)\n",
        "\n",
        "alpha_vals = [0.1, 1, 3, 10, 33, 100, 333, 1000, 3333, 10000, 33333]\n",
        "acc_train = np.zeros((len(alpha_vals),1))\n",
        "acc_val = np.zeros((len(alpha_vals),1))\n",
        "index = 0\n",
        "\n",
        "for l in alpha_vals:\n",
        "   acc_train[index], acc_val[index] = runLROVRmodel(10, dftrain, np.float(l))\n",
        "   print('For alpha value :', l,'we got an average f1-score on trainining set:', acc_train[index])\n",
        "   print('For alpha value :', l,'we got an average f1-score on validation set:', acc_val[index]) \n",
        "   print('')\n",
        "   index += 1\n",
        "   \n",
        "# penalty at which validation accuracy is maximum\n",
        "max_index_t  = np.argmax(acc_train[:,0])\n",
        "max_index_v  = np.argmax(acc_val[:,0])\n",
        "best_alpha_t = alpha_vals[max_index_t]\n",
        "best_alpha_v = alpha_vals[max_index_v]\n",
        "print(\"\")\n",
        "print(\"\"\"We have performed 10 random splits of training data into training (70%) and validation (30%) set.\n",
        "We then used these 10 sets of data to find the average validation performance P for both training sets and validation set\n",
        "which is the f1 score in our case.\n",
        "\n",
        "Best Alpha for training set gave maximum F1 score is: {}\"\"\".format(best_alpha_t))\n",
        "print(\"\"\"Best Alpha for validation that gave maximum F1 score is: {}\"\"\".format(best_alpha_v))\n",
        "\n",
        "#plot the accuracy curve\n",
        "plt.plot(range(0,len(alpha_vals)), acc_val[:,0], color='b', label='validation')\n",
        "plt.plot(range(0,len(alpha_vals)), acc_train[:,0], color='r', label='training')\n",
        "#replace the x-axis labels with penalty values\n",
        "plt.xticks(range(0,len(alpha_vals)), alpha_vals, rotation='vertical')\n",
        "\n",
        "#Highlight the best values of alpha and lambda\n",
        "plt.plot((max_index_v, max_index_v), (0, acc_val[max_index_v]), ls='dotted', color='b')\n",
        "plt.plot((max_index_t, max_index_t), (0, acc_train[max_index_t]), ls='dotted', color='r')\n",
        "\n",
        "#Set the y-axis from 0 to 1.0\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([0, 1.2])\n",
        "\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We are going to use P as f1-score since fID= 1\n",
            "For alpha value : 0.1 we got an average f1-score on trainining set: [1.]\n",
            "For alpha value : 0.1 we got an average f1-score on validation set: [0.84]\n",
            "\n",
            "For alpha value : 1 we got an average f1-score on trainining set: [1.]\n",
            "For alpha value : 1 we got an average f1-score on validation set: [0.84]\n",
            "\n",
            "For alpha value : 3 we got an average f1-score on trainining set: [1.]\n",
            "For alpha value : 3 we got an average f1-score on validation set: [0.84]\n",
            "\n",
            "For alpha value : 10 we got an average f1-score on trainining set: [1.]\n",
            "For alpha value : 10 we got an average f1-score on validation set: [0.84]\n",
            "\n",
            "For alpha value : 33 we got an average f1-score on trainining set: [1.]\n",
            "For alpha value : 33 we got an average f1-score on validation set: [0.84]\n",
            "\n",
            "For alpha value : 100 we got an average f1-score on trainining set: [1.]\n",
            "For alpha value : 100 we got an average f1-score on validation set: [0.84]\n",
            "\n",
            "For alpha value : 333 we got an average f1-score on trainining set: [1.]\n",
            "For alpha value : 333 we got an average f1-score on validation set: [0.83]\n",
            "\n",
            "For alpha value : 1000 we got an average f1-score on trainining set: [1.]\n",
            "For alpha value : 1000 we got an average f1-score on validation set: [0.85]\n",
            "\n",
            "For alpha value : 3333 we got an average f1-score on trainining set: [1.]\n",
            "For alpha value : 3333 we got an average f1-score on validation set: [0.85]\n",
            "\n",
            "For alpha value : 10000 we got an average f1-score on trainining set: [1.]\n",
            "For alpha value : 10000 we got an average f1-score on validation set: [0.85]\n",
            "\n",
            "For alpha value : 33333 we got an average f1-score on trainining set: [1.]\n",
            "For alpha value : 33333 we got an average f1-score on validation set: [0.87]\n",
            "\n",
            "\n",
            "We have performed 10 random splits of training data into training (70%) and validation (30%) set.\n",
            "We then used these 10 sets of data to find the average validation performance P for both training sets and validation set\n",
            "which is the f1 score in our case.\n",
            "\n",
            "Best Alpha for training set gave maximum F1 score is: 0.1\n",
            "Best Alpha for validation that gave maximum F1 score is: 33333\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYVll6Hng27O",
        "colab_type": "code",
        "colab": {},
        "outputId": "c8486ca5-fb56-48a0-91c1-d3281d02f8d5"
      },
      "source": [
        "#2.2b)Use the best alpha and all training data to build the final model and then evaluate the prediction performance \n",
        "#on test data and report the following:Â \n",
        "# a)The confusion matrix\n",
        "# b)Precision, recall and accuracy for each class.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "# import warnings filter\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "alpha=best_alpha_v\n",
        "\n",
        "clf = LogisticRegression(random_state=0, multi_class='ovr', C=1/alpha)\n",
        "ovr_model = clf.fit(features_train, labels_train)\n",
        "\n",
        "labels_predicted=ovr_model.predict(features_test)\n",
        "\n",
        "print('Accuracy of One Vs Rest is : '+ str(accuracy_score(labels_test,labels_predicted)))\n",
        "print('')\n",
        "print(classification_report(labels_test,labels_predicted,digits=3))\n",
        "\n",
        "confusion_matrix_ovr = confusion_matrix(labels_test,labels_predicted)\n",
        "print('Here is the confusion matrix')\n",
        "print('----------------------------')\n",
        "print(confusion_matrix_ovr)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of One Vs Rest is : 0.873015873015873\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.882     0.938     0.909        80\n",
            "           1      0.960     0.970     0.965        99\n",
            "           2      0.896     0.831     0.862        83\n",
            "           3      0.831     0.797     0.814        74\n",
            "           4      0.872     0.883     0.877        77\n",
            "           5      0.828     0.841     0.835        63\n",
            "           6      0.890     0.942     0.915        69\n",
            "           7      0.901     0.877     0.889        73\n",
            "           8      0.825     0.800     0.812        65\n",
            "           9      0.797     0.808     0.803        73\n",
            "\n",
            "   micro avg      0.873     0.873     0.873       756\n",
            "   macro avg      0.868     0.869     0.868       756\n",
            "weighted avg      0.873     0.873     0.873       756\n",
            "\n",
            "Here is the confusion matrix\n",
            "----------------------------\n",
            "[[75  1  1  2  0  0  1  0  0  0]\n",
            " [ 0 96  0  1  0  0  0  0  2  0]\n",
            " [ 1  0 69  1  1  1  6  0  3  1]\n",
            " [ 2  0  4 59  1  5  0  0  3  0]\n",
            " [ 0  0  0  1 68  0  0  0  0  8]\n",
            " [ 4  0  0  3  0 53  0  0  1  2]\n",
            " [ 2  0  0  0  1  0 65  0  1  0]\n",
            " [ 0  2  2  2  0  0  0 64  0  3]\n",
            " [ 1  1  0  2  2  5  1  0 52  1]\n",
            " [ 0  0  1  0  5  0  0  7  1 59]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28mqwbbig27U",
        "colab_type": "text"
      },
      "source": [
        "2.2b) Discuss if there is any sign of underfitting or overfitting with appropriate reasoning.\n",
        "\n",
        "There are no signs of over-fitting or under-fitting demostrated by this model.\n",
        "We could say this because the training set is showing good performance of 100% accuracy and the testing set also shows a good generalization of about 87%."
      ]
    }
  ]
}